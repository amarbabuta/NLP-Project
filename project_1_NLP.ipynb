{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name :-  Amar Babuta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Id:- 1031452"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The commented part of the code is for expermental purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nlp\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1168)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-0</th>\n",
       "      <th>train-1</th>\n",
       "      <th>train-2</th>\n",
       "      <th>train-3</th>\n",
       "      <th>train-4</th>\n",
       "      <th>train-5</th>\n",
       "      <th>train-6</th>\n",
       "      <th>train-7</th>\n",
       "      <th>train-8</th>\n",
       "      <th>train-9</th>\n",
       "      <th>...</th>\n",
       "      <th>train-1158</th>\n",
       "      <th>train-1159</th>\n",
       "      <th>train-1160</th>\n",
       "      <th>train-1161</th>\n",
       "      <th>train-1162</th>\n",
       "      <th>train-1163</th>\n",
       "      <th>train-1164</th>\n",
       "      <th>train-1165</th>\n",
       "      <th>train-1166</th>\n",
       "      <th>train-1167</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>why houston flooding isn‘t a sign of climate c...</td>\n",
       "      <td>The U.N. Intergovernmental Panel on Climate Ch...</td>\n",
       "      <td>Bureau Now Sets Strict Limits on Cooling\\nOVER...</td>\n",
       "      <td>The Dirty Extractive Underbelly of Clean Energ...</td>\n",
       "      <td>why climate change seems to have faded from th...</td>\n",
       "      <td>ipcc achieves zero credibility - our coalition...</td>\n",
       "      <td>Riots in Chile forced Chilean President Sebast...</td>\n",
       "      <td>claims of climate calamities made 100 months a...</td>\n",
       "      <td>Bureau’s Report Beyond Belief\\nThe release of ...</td>\n",
       "      <td>the climate change emergency is a pagan cult\\n...</td>\n",
       "      <td>...</td>\n",
       "      <td>Led by Colorado Attorney General (AG) Cynthia ...</td>\n",
       "      <td>earth day 2019 - not one single environmental ...</td>\n",
       "      <td>what navigator captain james cook might have s...</td>\n",
       "      <td>an avalanche of global warming alarmism is abo...</td>\n",
       "      <td>the world isn't going to halve co2 emissions b...</td>\n",
       "      <td>From Alberta to Australia, from Finland to Fra...</td>\n",
       "      <td>Lake Tahoe is enjoying a third consecutive yea...</td>\n",
       "      <td>New climate tests for pipelines are unnecessar...</td>\n",
       "      <td>Al Gore Claims Wind and Solar are Now Cheaper ...</td>\n",
       "      <td>This paper from Bruce Colbert, executive direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 train-0  \\\n",
       "text   why houston flooding isn‘t a sign of climate c...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-1  \\\n",
       "text   The U.N. Intergovernmental Panel on Climate Ch...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-2  \\\n",
       "text   Bureau Now Sets Strict Limits on Cooling\\nOVER...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-3  \\\n",
       "text   The Dirty Extractive Underbelly of Clean Energ...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-4  \\\n",
       "text   why climate change seems to have faded from th...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-5  \\\n",
       "text   ipcc achieves zero credibility - our coalition...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-6  \\\n",
       "text   Riots in Chile forced Chilean President Sebast...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-7  \\\n",
       "text   claims of climate calamities made 100 months a...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-8  \\\n",
       "text   Bureau’s Report Beyond Belief\\nThe release of ...   \n",
       "label                                                  1   \n",
       "\n",
       "                                                 train-9  ...  \\\n",
       "text   the climate change emergency is a pagan cult\\n...  ...   \n",
       "label                                                  1  ...   \n",
       "\n",
       "                                              train-1158  \\\n",
       "text   Led by Colorado Attorney General (AG) Cynthia ...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1159  \\\n",
       "text   earth day 2019 - not one single environmental ...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1160  \\\n",
       "text   what navigator captain james cook might have s...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1161  \\\n",
       "text   an avalanche of global warming alarmism is abo...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1162  \\\n",
       "text   the world isn't going to halve co2 emissions b...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1163  \\\n",
       "text   From Alberta to Australia, from Finland to Fra...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1164  \\\n",
       "text   Lake Tahoe is enjoying a third consecutive yea...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1165  \\\n",
       "text   New climate tests for pipelines are unnecessar...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1166  \\\n",
       "text   Al Gore Claims Wind and Solar are Now Cheaper ...   \n",
       "label                                                  1   \n",
       "\n",
       "                                              train-1167  \n",
       "text   This paper from Bruce Colbert, executive direc...  \n",
       "label                                                  1  \n",
       "\n",
       "[2 rows x 1168 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the data\n",
    "df=pd.read_json('train.json')\n",
    "#Get shape and head\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train-0</th>\n",
       "      <td>why houston flooding isn‘t a sign of climate c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-1</th>\n",
       "      <td>The U.N. Intergovernmental Panel on Climate Ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-2</th>\n",
       "      <td>Bureau Now Sets Strict Limits on Cooling\\nOVER...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-3</th>\n",
       "      <td>The Dirty Extractive Underbelly of Clean Energ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-4</th>\n",
       "      <td>why climate change seems to have faded from th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-1163</th>\n",
       "      <td>From Alberta to Australia, from Finland to Fra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-1164</th>\n",
       "      <td>Lake Tahoe is enjoying a third consecutive yea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-1165</th>\n",
       "      <td>New climate tests for pipelines are unnecessar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-1166</th>\n",
       "      <td>Al Gore Claims Wind and Solar are Now Cheaper ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-1167</th>\n",
       "      <td>This paper from Bruce Colbert, executive direc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text label\n",
       "train-0     why houston flooding isn‘t a sign of climate c...     1\n",
       "train-1     The U.N. Intergovernmental Panel on Climate Ch...     1\n",
       "train-2     Bureau Now Sets Strict Limits on Cooling\\nOVER...     1\n",
       "train-3     The Dirty Extractive Underbelly of Clean Energ...     1\n",
       "train-4     why climate change seems to have faded from th...     1\n",
       "...                                                       ...   ...\n",
       "train-1163  From Alberta to Australia, from Finland to Fra...     1\n",
       "train-1164  Lake Tahoe is enjoying a third consecutive yea...     1\n",
       "train-1165  New climate tests for pipelines are unnecessar...     1\n",
       "train-1166  Al Gore Claims Wind and Solar are Now Cheaper ...     1\n",
       "train-1167  This paper from Bruce Colbert, executive direc...     1\n",
       "\n",
       "[1168 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transpose the dataframe\n",
    "df1=df.T\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the External dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4168, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISLAMABAD: Finance Minister Ishaq Dar presente...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISLAMABAD: As Senate Chairman Raza Rabbani rul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEW DELHI: India unveiled a fire-fighting budg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strong&gt;ISLAMABAD: Prime Minister Nawaz Sharif ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>strong&gt;LONDON/NEW DELHI/MOSCOW: A multi-billio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  ISLAMABAD: Finance Minister Ishaq Dar presente...      0\n",
       "1  ISLAMABAD: As Senate Chairman Raza Rabbani rul...      0\n",
       "2  NEW DELHI: India unveiled a fire-fighting budg...      0\n",
       "3  strong>ISLAMABAD: Prime Minister Nawaz Sharif ...      0\n",
       "4  strong>LONDON/NEW DELHI/MOSCOW: A multi-billio...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the data\n",
    "df2=pd.read_json('external_data.json')\n",
    "#Get shape and head\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concating the train and external data in new data frame\n",
    "df4=pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why houston flooding isn‘t a sign of climate c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.N. Intergovernmental Panel on Climate Ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bureau Now Sets Strict Limits on Cooling\\nOVER...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dirty Extractive Underbelly of Clean Energ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why climate change seems to have faded from th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>veteran martinez wins thai title conchita mart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>everton s weir cools euro hopes everton defend...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>tottenham bid &amp;#163;8m for forest duo nottingh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>solskjaer raises hopes of return manchester un...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5335</th>\n",
       "      <td>blunkett hints at election call ex-home secret...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5336 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     why houston flooding isn‘t a sign of climate c...      1\n",
       "1     The U.N. Intergovernmental Panel on Climate Ch...      1\n",
       "2     Bureau Now Sets Strict Limits on Cooling\\nOVER...      1\n",
       "3     The Dirty Extractive Underbelly of Clean Energ...      1\n",
       "4     why climate change seems to have faded from th...      1\n",
       "...                                                 ...    ...\n",
       "5331  veteran martinez wins thai title conchita mart...      0\n",
       "5332  everton s weir cools euro hopes everton defend...      0\n",
       "5333  tottenham bid &#163;8m for forest duo nottingh...      0\n",
       "5334  solskjaer raises hopes of return manchester un...      0\n",
       "5335  blunkett hints at election call ex-home secret...      0\n",
       "\n",
       "[5336 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting to orient records\n",
    "df4.to_json('newtrain.json', orient='records')\n",
    "newtrain = pd.read_json('newtrain.json')\n",
    "newtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the labels from newtrain\n",
    "labels=newtrain.label\n",
    "labels.head()\n",
    "labels=labels.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "1168  ISLAMABAD: Finance Minister Ishaq Dar presente...      0\n",
      "1169  ISLAMABAD: As Senate Chairman Raza Rabbani rul...      0\n",
      "1170  NEW DELHI: India unveiled a fire-fighting budg...      0\n",
      "1171  strong>ISLAMABAD: Prime Minister Nawaz Sharif ...      0\n",
      "1172  strong>LONDON/NEW DELHI/MOSCOW: A multi-billio...      0\n",
      "...                                                 ...    ...\n",
      "5331  veteran martinez wins thai title conchita mart...      0\n",
      "5332  everton s weir cools euro hopes everton defend...      0\n",
      "5333  tottenham bid &#163;8m for forest duo nottingh...      0\n",
      "5334  solskjaer raises hopes of return manchester un...      0\n",
      "5335  blunkett hints at election call ex-home secret...      0\n",
      "\n",
      "[4168 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "zero = newtrain['label'] == 0\n",
    "zero2 = newtrain[zero]\n",
    "print(zero2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dev-0</th>\n",
       "      <td>Are Climate Models Overpredicting Global Warmi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-1</th>\n",
       "      <td>The latest National Climate Assessment, releas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-2</th>\n",
       "      <td>Climate Strike Kids Cool on Real Action\\nA pop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-3</th>\n",
       "      <td>Morrison a ‘predatory’ centrist on climate pol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-4</th>\n",
       "      <td>CNN’s Town Hall Full of Hot Air\\nSixty-three s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-95</th>\n",
       "      <td>Texas Man Guilty of Kidnapping Still-Missing W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-96</th>\n",
       "      <td>Climate campaigners condemn 'insidious' cockta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-97</th>\n",
       "      <td>Forget the climate emergency: they need a Pied...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-98</th>\n",
       "      <td>Why don’t we treat the climate crisis with the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev-99</th>\n",
       "      <td>Even if You Buy the Science, the Policy Still ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text label\n",
       "dev-0   Are Climate Models Overpredicting Global Warmi...     1\n",
       "dev-1   The latest National Climate Assessment, releas...     1\n",
       "dev-2   Climate Strike Kids Cool on Real Action\\nA pop...     1\n",
       "dev-3   Morrison a ‘predatory’ centrist on climate pol...     0\n",
       "dev-4   CNN’s Town Hall Full of Hot Air\\nSixty-three s...     1\n",
       "...                                                   ...   ...\n",
       "dev-95  Texas Man Guilty of Kidnapping Still-Missing W...     0\n",
       "dev-96  Climate campaigners condemn 'insidious' cockta...     0\n",
       "dev-97  Forget the climate emergency: they need a Pied...     1\n",
       "dev-98  Why don’t we treat the climate crisis with the...     0\n",
       "dev-99  Even if You Buy the Science, the Policy Still ...     1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df=pd.read_json('dev.json')\n",
    "#making transpose of dev_df\n",
    "dev_df1=dev_df.T\n",
    "dev_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are Climate Models Overpredicting Global Warmi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The latest National Climate Assessment, releas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Climate Strike Kids Cool on Real Action\\nA pop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morrison a ‘predatory’ centrist on climate pol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN’s Town Hall Full of Hot Air\\nSixty-three s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Texas Man Guilty of Kidnapping Still-Missing W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Climate campaigners condemn 'insidious' cockta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Forget the climate emergency: they need a Pied...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Why don’t we treat the climate crisis with the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Even if You Buy the Science, the Policy Still ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   Are Climate Models Overpredicting Global Warmi...      1\n",
       "1   The latest National Climate Assessment, releas...      1\n",
       "2   Climate Strike Kids Cool on Real Action\\nA pop...      1\n",
       "3   Morrison a ‘predatory’ centrist on climate pol...      0\n",
       "4   CNN’s Town Hall Full of Hot Air\\nSixty-three s...      1\n",
       "..                                                ...    ...\n",
       "95  Texas Man Guilty of Kidnapping Still-Missing W...      0\n",
       "96  Climate campaigners condemn 'insidious' cockta...      0\n",
       "97  Forget the climate emergency: they need a Pied...      1\n",
       "98  Why don’t we treat the climate crisis with the...      0\n",
       "99  Even if You Buy the Science, the Policy Still ...      1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting to orient records\n",
    "dev_df1.to_json('dev-1.json', orient='records')\n",
    "dev1 = pd.read_json('dev-1.json')\n",
    "dev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the labels from dev dataset\n",
    "dev_labels=dev_df1.label\n",
    "dev_labels.head()\n",
    "dev_labels=dev_labels.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test-0</th>\n",
       "      <td>Marketing Executive Somehow Unable To Stop, Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-1</th>\n",
       "      <td>WASHINGTON  Republican presidential front-runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-2</th>\n",
       "      <td>Turkey Could Be Taking A Big Step Backwards In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-3</th>\n",
       "      <td>New York  Keep your kids away from these Santa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-4</th>\n",
       "      <td>Global warming is causing more frequent heatwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-1405</th>\n",
       "      <td>Warwick asks voters to back radical council ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-1406</th>\n",
       "      <td>Pele, Diego Maradona and Franz Beckenbaeur wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-1407</th>\n",
       "      <td>Electric cars produce less CO2 than petrol veh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-1408</th>\n",
       "      <td>Latest Climate Report Feeds into Alarmist Fear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-1409</th>\n",
       "      <td>Sarah Palin's teleprompter did freeze in Iowa ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1410 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text\n",
       "test-0     Marketing Executive Somehow Unable To Stop, Pr...\n",
       "test-1     WASHINGTON  Republican presidential front-runn...\n",
       "test-2     Turkey Could Be Taking A Big Step Backwards In...\n",
       "test-3     New York  Keep your kids away from these Santa...\n",
       "test-4     Global warming is causing more frequent heatwa...\n",
       "...                                                      ...\n",
       "test-1405  Warwick asks voters to back radical council ta...\n",
       "test-1406  Pele, Diego Maradona and Franz Beckenbaeur wil...\n",
       "test-1407  Electric cars produce less CO2 than petrol veh...\n",
       "test-1408  Latest Climate Report Feeds into Alarmist Fear...\n",
       "test-1409  Sarah Palin's teleprompter did freeze in Iowa ...\n",
       "\n",
       "[1410 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=pd.read_json('test-unlabelled.json')\n",
    "#making transpose of test_df\n",
    "test_df1=test_df.T\n",
    "test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marketing Executive Somehow Unable To Stop, Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON  Republican presidential front-runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Turkey Could Be Taking A Big Step Backwards In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New York  Keep your kids away from these Santa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global warming is causing more frequent heatwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>Warwick asks voters to back radical council ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>Pele, Diego Maradona and Franz Beckenbaeur wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>Electric cars produce less CO2 than petrol veh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>Latest Climate Report Feeds into Alarmist Fear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>Sarah Palin's teleprompter did freeze in Iowa ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1410 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     Marketing Executive Somehow Unable To Stop, Pr...\n",
       "1     WASHINGTON  Republican presidential front-runn...\n",
       "2     Turkey Could Be Taking A Big Step Backwards In...\n",
       "3     New York  Keep your kids away from these Santa...\n",
       "4     Global warming is causing more frequent heatwa...\n",
       "...                                                 ...\n",
       "1405  Warwick asks voters to back radical council ta...\n",
       "1406  Pele, Diego Maradona and Franz Beckenbaeur wil...\n",
       "1407  Electric cars produce less CO2 than petrol veh...\n",
       "1408  Latest Climate Report Feeds into Alarmist Fear...\n",
       "1409  Sarah Palin's teleprompter did freeze in Iowa ...\n",
       "\n",
       "[1410 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting to orient records\n",
    "test_df1.to_json('test-unlabelled1.json', orient='records')\n",
    "test1 = pd.read_json('test-unlabelled1.json')\n",
    "test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Feature selection Approach and it's execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Approach A) Using normal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Normal features for newtrain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing newtrain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in newtrain['text']:\n",
    "#     sentences=nltk.sent_tokenize(word)\n",
    "#     print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Punctuation from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punct(text):\n",
    "#     text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "#     text = re.sub('[0-9]+', '', text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of stopwords from the text by converting to lowercase and lemmatizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_str = []\n",
    "# str1 =\" \"\n",
    "# for word in newtrain['text']:\n",
    "#     sentences=nltk.sent_tokenize(word)\n",
    "#     for item in range(len(sentences)):\n",
    "#         words = nltk.word_tokenize(remove_punct(sentences[item]).lower())\n",
    "#         words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "#         sentences[item] = ' '.join(words)\n",
    "#     string_1 = str1.join(sentences)\n",
    "#     new_str.append(string_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = pd.DataFrame(new_str)\n",
    "# newtrain['preprocessed_text']= new\n",
    "# newtrain = newtrain[['text','preprocessed_text','label']]\n",
    "# newtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Normal features for dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in dev1['text']:\n",
    "#     sentences=nltk.sent_tokenize(word)\n",
    "#     print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of stopwords, punctuation from the text by converting to lowercase and lemmatizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_str1 = []\n",
    "# str2 =\" \"\n",
    "# for word in dev1['text']:\n",
    "#     sentences1=nltk.sent_tokenize(word)\n",
    "#     for item in range(len(sentences1)):\n",
    "#         words = nltk.word_tokenize(remove_punct(sentences1[item]).lower())\n",
    "#         words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "#         sentences1[item] = ' '.join(words)\n",
    "#     string_2 = str1.join(sentences1)\n",
    "#     new_str1.append(string_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev = pd.DataFrame(new_str1)\n",
    "# dev1['preprocessed_text']= dev\n",
    "# dev1= dev1[['text','preprocessed_text','label']]\n",
    "# dev1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Normal features for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in test1['text']:\n",
    "#     sentences=nltk.sent_tokenize(word)\n",
    "#     print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of stopwords, punctuation from the text by converting to lowercase and lemmatizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_str2 = []\n",
    "# str3 =\" \"\n",
    "# for word in test1['text']:\n",
    "#     sentences2=nltk.sent_tokenize(word)\n",
    "#     for item in range(len(sentences2)):\n",
    "#         words = nltk.word_tokenize(remove_punct(sentences2[item]).lower())\n",
    "#         words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "#         sentences2[item] = ' '.join(words)\n",
    "#     string_3 = str3.join(sentences2)\n",
    "#     new_str2.append(string_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.DataFrame(new_str2)\n",
    "# test1['preprocessed_text']= test\n",
    "# test1= test1[['text','preprocessed_text']]\n",
    "# test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applying normal features to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split the dataset\n",
    "# x_train,x_test,y_train,y_test=train_test_split(newtrain['preprocessed_text'], labels, test_size=0.3, random_state=7)\n",
    "# y_train=y_train.astype('int')\n",
    "# y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb = BernoulliNB()\n",
    "# bnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_predict = bnb.predict(x_test)\n",
    "# print(\"accuracy: {}%\".format(round(accuracy_score(y_test, bnb_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach b) YAKE(Yet Another Keyword Extraction) for Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Manipulating newtrain dataset to apply YAKE \n",
    "\n",
    "# newtrain.insert(0, 'ID', range(0, 0 + len(newtrain)))\n",
    "# newtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Applying YAKE on newtrain dataset\n",
    "\n",
    "\n",
    "# import yake\n",
    "\n",
    "# final={}\n",
    "# for i in range(len(newtrain['text'])):\n",
    "#     trial = []\n",
    "#     id = newtrain['ID'][i]\n",
    "#     text = newtrain['text'][i]\n",
    "#     # assuming default parameters\n",
    "#     simple_kwextractor = yake.KeywordExtractor()\n",
    "#     keywords = simple_kwextractor.extract_keywords(text)\n",
    "    \n",
    "#     # for kw in keywords:\n",
    "#     # \tprint(kw)\n",
    "    \n",
    "#     # specifying parameters\n",
    "#     max_ngram_size = 3\n",
    "#     custom_kwextractor = yake.KeywordExtractor(lan=\"en\", n = max_ngram_size, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None)\n",
    "    \n",
    "#     keywords = custom_kwextractor.extract_keywords(text)\n",
    "    \n",
    "#     for (a,b) in keywords:\n",
    "#         if b<0.6:\n",
    "#             #print (a)\n",
    "#             trial.append(a)\n",
    "            \n",
    "#     if id not in final:\n",
    "#         final[id]=[]\n",
    "#     final[id].append(trial)\n",
    "\n",
    "# print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_frame = pd.DataFrame(final)\n",
    "# train_frame= train_frame.T\n",
    "# newtrain['keyword'] = train_frame\n",
    "# newtrain = newtrain[['ID','text','keyword' , 'label']]\n",
    "# newtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_to_str(req):\n",
    "#     return ' '.join(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtrain['keyword'] =newtrain['keyword'].apply(list_to_str)\n",
    "#newtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unique(text):\n",
    "#     for word in text:\n",
    "#         text = text.split()\n",
    "#         text = set(text)\n",
    "#         return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtrain['title_unq'] = newtrain['keyword'].apply(lambda x: unique(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_to_lst(req):\n",
    "#     return list(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtrain['title_unq'] = newtrain['title_unq'].apply(set_to_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtrain['title_unq'] =newtrain['title_unq'].apply(list_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtrain=newtrain[['ID','text','keyword','title_unq','label']]\n",
    "#newtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #manipulating Dev Dataset\n",
    "# dev1.insert(0, 'ID', range(0, 0 + len(dev1)))\n",
    "# dev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Applying YAKE on Dev Dataset\n",
    "\n",
    "# import yake\n",
    "\n",
    "# final={}\n",
    "# for i in range(len(dev1['text'])):\n",
    "#     trial = []\n",
    "#     id = dev1['ID'][i]\n",
    "#     text = dev1['text'][i]\n",
    "#     # assuming default parameters\n",
    "#     simple_kwextractor = yake.KeywordExtractor()\n",
    "#     keywords = simple_kwextractor.extract_keywords(text)\n",
    "    \n",
    "#     # for kw in keywords:\n",
    "#     # \tprint(kw)\n",
    "    \n",
    "#     # specifying parameters\n",
    "#     max_ngram_size = 3\n",
    "#     custom_kwextractor = yake.KeywordExtractor(lan=\"en\", n = max_ngram_size, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None)\n",
    "    \n",
    "#     keywords = custom_kwextractor.extract_keywords(text)\n",
    "    \n",
    "#     for (a,b) in keywords:\n",
    "#         if b<0.6:\n",
    "#             #print (a)\n",
    "#             trial.append(a)\n",
    "            \n",
    "#     if id not in final:\n",
    "#         final[id]=[]\n",
    "#     final[id].append(trial)\n",
    "\n",
    "# print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_frame = pd.DataFrame(final)\n",
    "# dic_frame= dic_frame.T\n",
    "# dev1['keyword'] = dic_frame\n",
    "# dev1 = dev1[['ID','text','keyword' , 'label']]\n",
    "# dev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev1['keyword'] =dev1['keyword'].apply(list_to_str)\n",
    "# dev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev1['title_unq'] = dev1['keyword'].apply(lambda x: unique(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev1['title_unq'] = dev1['title_unq'].apply(set_to_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev1['title_unq'] =dev1['title_unq'].apply(list_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev1=dev1[['ID','text','keyword','title_unq','label']]\n",
    "# dev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Manipulating Test Dataset\n",
    "# test1.insert(0, 'ID', range(0, 0 + len(test1)))\n",
    "# test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Applying YAKE on test dataset\n",
    "\n",
    "# import yake\n",
    "\n",
    "# final={}\n",
    "# for i in range(len(newtrain['text'])):\n",
    "#     trial = []\n",
    "#     id = newtrain['ID'][i]\n",
    "#     text = newtrain['text'][i]\n",
    "#     # assuming default parameters\n",
    "#     simple_kwextractor = yake.KeywordExtractor()\n",
    "#     keywords = simple_kwextractor.extract_keywords(text)\n",
    "    \n",
    "#     # for kw in keywords:\n",
    "#     # \tprint(kw)\n",
    "    \n",
    "#     # specifying parameters\n",
    "#     max_ngram_size = 3\n",
    "#     custom_kwextractor = yake.KeywordExtractor(lan=\"en\", n = max_ngram_size, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None)\n",
    "    \n",
    "#     keywords = custom_kwextractor.extract_keywords(text)\n",
    "    \n",
    "#     for (a,b) in keywords:\n",
    "#         if b<0.6:\n",
    "#             #print (a)\n",
    "#             trial.append(a)\n",
    "            \n",
    "#     if id not in final:\n",
    "#         final[id]=[]\n",
    "#     final[id].append(trial)\n",
    "\n",
    "# print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_frame = pd.DataFrame(final)\n",
    "# test_frame= test_frame.T\n",
    "# test1['keyword'] = test_frame\n",
    "# test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1['keyword'] = test1['keyword'].apply(list_to_str)\n",
    "# test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1['title_unq'] = test1['keyword'].apply(lambda x: unique(x))\n",
    "# test1['title_unq'] = test1['title_unq'].apply(set_to_lst)\n",
    "# test1['title_unq'] =test1['title_unq'].apply(list_to_str)\n",
    "# test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applying YAKE with TF-IDF for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize a TfidfVectorizer\n",
    "# tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# #Fit and transform train set\n",
    "# tfidf_train=tfidf_vectorizer.fit_transform(newtrain['title_unq'])\n",
    "# tfidf_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying extracted feature to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split the dataset\n",
    "# x_train,x_test,y_train,y_test=train_test_split(tfidf_train, labels, test_size=0.3, random_state=7)\n",
    "# y_train=y_train.astype('int')\n",
    "# y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_svm=LinearSVC()\n",
    "# linear_svm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_svm_predict = linear_svm.predict(x_test)\n",
    "# print(\"accuracy: {}%\".format(round(accuracy_score(y_test, lin_svm_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Approach c) TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5336x47556 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 793776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "#Fit and transform train set\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(newtrain['text'])\n",
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "x_train,x_test,y_train,y_test=train_test_split(tfidf_train, labels, test_size=0.3, random_state=7)\n",
    "y_train=y_train.astype('int')\n",
    "y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing on Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb = MultinomialNB()\n",
    "# mnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb_predict = mnb.predict(x_test)\n",
    "# print(\"accuracy: {}%\".format(round(accuracy_score(y_test, mnb_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing on Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC()\n",
    "# svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_predict = svc.predict(x_test)\n",
    "# print(\"accuracy: {}%\".format(round(accuracy_score(y_test, svc_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning of SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize a TfidfVectorizer\n",
    "# tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# #Fit and transform train set\n",
    "# tfidf_train=tfidf_vectorizer.fit_transform(dev1['text'])\n",
    "# tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split the dataset\n",
    "# x_train,x_test,y_train,y_test=train_test_split(tfidf_train, dev1.label, test_size=0.3, random_state=7)\n",
    "# y_train=y_train.astype('int')\n",
    "# y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'C': [0.1,1, 10, 100 , 1000], \n",
    "#                 'gamma': [1,0.1,0.01,0.001 , 0.0001],\n",
    "#                 'kernel': ['rbf', 'poly', 'sigmoid' , 'linear']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(svm,param_grid,refit=True,verbose=2)\n",
    "# grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying Optimized Parameter on newtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize a TfidfVectorizer\n",
    "# tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# #Fit and transform train set\n",
    "# tfidf_train=tfidf_vectorizer.fit_transform(newtrain['text'])\n",
    "# tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split the dataset\n",
    "# x_train,x_test,y_train,y_test=train_test_split(tfidf_train, labels, test_size=0.3, random_state=7)\n",
    "# y_train=y_train.astype('int')\n",
    "# y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(C=10, gamma=1, kernel='sigmoid')\n",
    "# svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_predict = svc.predict(x_test)\n",
    "# print(\"accuracy: {}%\".format(round(accuracy_score(y_test, svc_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach d) Training the model with ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize a TfidfVectorizer\n",
    "# tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# #Fit and transform train set\n",
    "# tfidf_train=tfidf_vectorizer.fit_transform(newtrain['text'])\n",
    "# tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split the dataset\n",
    "# x_train,x_test,y_train,y_test=train_test_split(tfidf_train, labels, test_size=0.3, random_state=7)\n",
    "# y_train=y_train.astype('int')\n",
    "# y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble=VotingClassifier(estimators=[('LR', lr), ('SVC', svc)], \n",
    "#                        voting='hard', weights=[1,2]).fit(x_train,y_train)\n",
    "# print('The accuracy for Logistic Regression and Support Vector Machines is:',ensemble.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_predict = ensemble.predict(x_test)\n",
    "# print(\"accuracy: {}%\".format(round(accuracy_score(y_test, ensemble_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5336x47556 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 793776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "#Fit and transform train set\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(newtrain['text'])\n",
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "x_train,x_test,y_train,y_test=train_test_split(tfidf_train, labels, test_size=0.3, random_state=7)\n",
    "y_train=y_train.astype('int')\n",
    "y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMAR\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 97.31%\n"
     ]
    }
   ],
   "source": [
    "lr_predict = lr.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, lr_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x9606 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26064 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "#Fit and transform train set\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(dev1['text'])\n",
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "x_train,x_test,y_train,y_test=train_test_split(tfidf_train, dev1.label, test_size=0.3, random_state=7)\n",
    "y_train=y_train.astype('int')\n",
    "y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 86.67%\n"
     ]
    }
   ],
   "source": [
    "lr_predict = lr.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, lr_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMAR\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n",
      "Best Score:  0.9519480519480518\n",
      "Best Params:  {'C': 100, 'class_weight': {1: 0.7, 0: 0.3}, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 384 out of 384 | elapsed:    1.7s finished\n",
      "C:\\Users\\AMAR\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\AMAR\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
    "solver = ['liblinear', 'saga']\n",
    "\n",
    "param_grid = dict(penalty=penalty,\n",
    "                  C=C,\n",
    "                  class_weight=class_weight,\n",
    "                  solver=solver)\n",
    "\n",
    "grid = GridSearchCV(estimator=lr,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight={1: 0.7, 0: 0.3}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "          solver='saga', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1000, class_weight={1: 0.7, 0: 0.3}, penalty='l1',solver='saga')\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "lr_predict = lr.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, lr_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n",
      "\n",
      " Confusion Matrix : \n",
      "\n",
      "[[13  2]\n",
      " [ 3 12]]\n",
      "\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.8125\n",
      "Recall    = 0.8666666666666667\n",
      "F1        = 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "score=accuracy_score(y_test,lr_predict)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "\n",
    "cm=confusion_matrix(y_test,lr_predict, labels=[1,0])\n",
    "print (\"\\n Confusion Matrix : \\n\")\n",
    "print (cm)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_test,lr_predict, pos_label=1, average=\"binary\");\n",
    "\n",
    "print(\"\\nPerformance on the positive class (documents with misinformation):\")\n",
    "print(\"Precision =\", p)\n",
    "print(\"Recall    =\", r)\n",
    "print(\"F1        =\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying Optimized Parameter on newtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5336x47556 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 793776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "#Fit and transform train set\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(newtrain['text'])\n",
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "x_train,x_test,y_train,y_test=train_test_split(tfidf_train, labels, test_size=0.3, random_state=7)\n",
    "y_train=y_train.astype('int')\n",
    "y_test=y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight={1: 0.7, 0: 0.3}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "          solver='saga', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1000, class_weight={1: 0.7, 0: 0.3}, penalty='l1',solver='saga')\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 99.44%\n"
     ]
    }
   ],
   "source": [
    "lr_predict = lr.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, lr_predict)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting the labels of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tfidf_train\n",
    "y_train=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=test1['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pred=tfidf_vectorizer.transform(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred=lr.predict(tfidf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1['label']= lr_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TestOutput=test_df1[['label']].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TestOutput=df_TestOutput.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TestOutput.to_json ('test-output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
